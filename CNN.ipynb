{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1633,
     "status": "ok",
     "timestamp": 1742456491471,
     "user": {
      "displayName": "Georgii Miakishev",
      "userId": "00896216475612259069"
     },
     "user_tz": -60
    },
    "id": "NLYDTWbMqPtn",
    "outputId": "3aa91267-148b-4784-8510-c5c030d9661a"
   },
   "outputs": [],
   "source": [
    "# FUNCTIONSpiup\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "def saveImages(data, path,epoch = \"\"):\n",
    "    i = 0\n",
    "    add = \"\"\n",
    "\n",
    "    for array in data:\n",
    "        if len(array.shape) == 2:\n",
    "                array2 = np.asarray(array*255 , np.uint8)\n",
    "                array2 = np.reshape(array2,[imwidth,imwidth])\n",
    "                add = \"x\"+str(epoch).zfill(3)\n",
    "                im = Image.fromarray(array2)\n",
    "                im.save(os.path.join(path,add+str(i)+\".png\"))\n",
    "                i = i+1\n",
    "        if len(array.shape)==3:\n",
    "            if array.shape[2] == 1:\n",
    "                array = array[:, :, 0]\n",
    "                add = \"z\"\n",
    "\n",
    "            im = Image.fromarray(array)\n",
    "            im.save(os.path.join(path,add+str(i)+\".png\"))\n",
    "            i = i+1\n",
    "\n",
    "def read_images(input_paths,label_paths):\n",
    "    \"\"\" read filepaths and return decoded images \"\"\"\n",
    "    input_content = tf.io.read_file(input_paths)\n",
    "    label_content = tf.io.read_file(label_paths)\n",
    "    input_images = tf.image.decode_png(input_content, channels=3)\n",
    "    labels = tf.image.decode_png(label_content, channels=1)\n",
    "    return input_images, labels\n",
    "\n",
    "def create_stars_data(input_image, label):\n",
    "    \"\"\" manipulates images as needed for the stars (and similar) dataset \"\"\"\n",
    "    input_image = tf.divide(input_image, np.uint8(255), name=\"normalize_x\")\n",
    "    input_image = tf.image.resize(input_image, [imwidth, imwidth])\n",
    "    #input_image = tf.reshape(input_image,[3,imwidth,imwidth])\n",
    "\n",
    "    label = tf.divide(label, np.uint8(255), name=\"normalize_y\")\n",
    "    label = tf.image.resize(label, [imwidth, imwidth])\n",
    "    label = tf.round(label, name=\"round_y\")\n",
    "    # label = tf.reshape(label,[1,imwidth*imwidth])\n",
    "    return input_image, label\n",
    "\n",
    "def showImagesDataColor(data):\n",
    "            fig=plt2.figure(figsize=(16, 16))\n",
    "            columns = len(data)\n",
    "            rows = 1\n",
    "            i=0\n",
    "            for array in data:\n",
    "                i = i+1\n",
    "                array2 = np.asarray(array*255 , np.uint8)\n",
    "                #array2 = np.reshape(array2,[imwidth,imwidth,3])\n",
    "                array2 = np.reshape(array2,[imwidth,imwidth,3])\n",
    "\n",
    "                fig.add_subplot(rows, columns,i)\n",
    "                plt2.imshow(array2) #,cmap='sRGB1')\n",
    "            plt2.show()\n",
    "\n",
    "def showImagesDataGrey(data):\n",
    "            fig=plt2.figure(figsize=(16, 16))\n",
    "            columns = len(data)\n",
    "            rows = 1\n",
    "            i=0\n",
    "            for array in data:\n",
    "                i = i+1\n",
    "                array2 = np.asarray(array*255 , np.uint8)\n",
    "                array2 = np.reshape(array2,[imwidth,imwidth])\n",
    "\n",
    "                fig.add_subplot(rows, columns,i)\n",
    "                plt2.imshow(array2,cmap='gray')\n",
    "            plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yUEJxtfZ8wc_",
    "outputId": "14f422ef-070c-4f4b-9f34-2648d9bc2320"
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# zip_path = \"Stars.zip\"  # Replace with the path to your zip file\n",
    "# extract_path = \"extracted_folder\"  # Folder where files will be extracted\n",
    "\n",
    "# # Ensure the extraction folder exists\n",
    "# os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "# # Unzip the file\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtmWKvbA8t8M"
   },
   "source": [
    ".ZI# ConvStarNet - Visualizing Weights and Activation Maps\n",
    "\n",
    "In this session, we revisit the star identification task seen earlier where we will use CNNs instead of Dense layers. The notebook therefore follows the same structure as the earlier notebook.\n",
    "\n",
    "\n",
    "Input:\n",
    "![2018-11-19_19h52_19.png](attachment:2018-11-19_19h52_19.png)\n",
    "Output:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-4u2zp98t8N"
   },
   "source": [
    "Our neural network should take some image data x and output some image data y with the recognized stars. For training the network we use the star dataset given with this lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mPKHwos8t8N"
   },
   "source": [
    "![2018-12-18_09h33_42.png](attachment:2018-12-18_09h33_42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSDKxguE8t8N"
   },
   "source": [
    "## We have to import some libraries and functions first.\n",
    "Needed libraries are:\n",
    "pillow, tensorflow, numpy, matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "wm50PRqv8t8N"
   },
   "outputs": [],
   "source": [
    "import os, platform, sys, glob, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDIKcgNB8t8O"
   },
   "source": [
    "We first set up some general hyperparameters for the training and preprocessing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "vbCmvVmA8t8O"
   },
   "outputs": [],
   "source": [
    "###### Hyperparameter\n",
    "learnrate = 0.0001    # Learning Rate\n",
    "batchsize = 10        # Images per batch\n",
    "ep_lim = 100           # Max number of epochs\n",
    "imwidth = 25          # Image dimension used for training, smaller images computer faster\n",
    "                      # big impact on computation time, original image is 75x75\n",
    "imwidth = imwidth # Set width for imf (image functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5813Kq1I8t8O"
   },
   "source": [
    "## Input data\n",
    "We search the folders containing the data and create two file list, one containing the x and one containing the y file paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1742459647384,
     "user": {
      "displayName": "Medhansh Agarwal",
      "userId": "03876177846036089524"
     },
     "user_tz": -60
    },
    "id": "u9i53WDE8t8O",
    "outputId": "43cedbf0-ce06-445e-b1e5-a72e7342987b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  2350  images\n"
     ]
    }
   ],
   "source": [
    "#### Lisiting all Image files\n",
    "x_path = \"extracted_folder/Stars/x\"\n",
    "y_path = \"extracted_folder\\Stars\\y\"\n",
    "file_list_x = []\n",
    "file_list_y = []\n",
    "file_list_x += sorted(glob.glob(x_path + \"/**/*.png\", recursive=True))\n",
    "file_list_y += sorted(glob.glob(y_path + \"/**/*.png\", recursive=True))\n",
    "print(\"Found \",len(file_list_x),\" images\")\n",
    "assert len(file_list_x) == 2350 # If this line failes you did something wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2zkNRQ88t8O"
   },
   "source": [
    "## Tensorflow Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7453,
     "status": "ok",
     "timestamp": 1742459654839,
     "user": {
      "displayName": "Medhansh Agarwal",
      "userId": "03876177846036089524"
     },
     "user_tz": -60
    },
    "id": "DdH1MCEo8t8O",
    "outputId": "70638f29-bde3-48c6-8846-c8392764060f"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m file_x \u001b[38;5;241m=\u001b[39m file_list_x[i]\n\u001b[0;32m      5\u001b[0m file_y \u001b[38;5;241m=\u001b[39m file_list_y[i]\n\u001b[1;32m----> 6\u001b[0m X,Y \u001b[38;5;241m=\u001b[39m \u001b[43mread_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m X,Y \u001b[38;5;241m=\u001b[39m create_stars_data(X,Y)\n\u001b[0;32m      8\u001b[0m XX\u001b[38;5;241m.\u001b[39mappend(X\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[1;32mIn[38], line 32\u001b[0m, in \u001b[0;36mread_images\u001b[1;34m(input_paths, label_paths)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_images\u001b[39m(input_paths,label_paths):\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" read filepaths and return decoded images \"\"\"\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     input_content \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     label_content \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_file(label_paths)\n\u001b[0;32m     34\u001b[0m     input_images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mdecode_png(input_content, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\medha\\OneDrive\\Desktop\\AE2224-TAS\\TAS-AE2224-I\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\io_ops.py:134\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.read_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.read_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_file\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_file\u001b[39m(filename, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reads the contents of file.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m  This operation returns a tensor with the entire contents of the input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    A tensor of dtype \"string\", with the file contents.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_io_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\medha\\OneDrive\\Desktop\\AE2224-TAS\\TAS-AE2224-I\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:583\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m    581\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 583\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_file_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m    586\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\medha\\OneDrive\\Desktop\\AE2224-TAS\\TAS-AE2224-I\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:606\u001b[0m, in \u001b[0;36mread_file_eager_fallback\u001b[1;34m(filename, name, ctx)\u001b[0m\n\u001b[0;32m    604\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [filename]\n\u001b[0;32m    605\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 606\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[0;32m    609\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[0;32m    610\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadFile\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[1;32mc:\\Users\\medha\\OneDrive\\Desktop\\AE2224-TAS\\TAS-AE2224-I\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "XX = []\n",
    "YY = []\n",
    "for i in range(len(file_list_x)):\n",
    "    file_x = file_list_x[i]\n",
    "    file_y = file_list_y[i]\n",
    "    X,Y = read_images(file_x,file_y)\n",
    "    X,Y = create_stars_data(X,Y)\n",
    "    XX.append(X.numpy())\n",
    "    YY.append(Y.numpy())\n",
    "XX = np.array(XX)\n",
    "YY = np.array(YY)\n",
    "XX = np.reshape(XX, (-1,25,25,3))\n",
    "YY = np.reshape(YY, (-1,25,25,1))\n",
    "print(XX.shape)\n",
    "print(YY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FFx7A1y8t8P"
   },
   "source": [
    "## Defining our Neural Network and initial values for all Variables\n",
    "Now we can define our neural network. This time, we did not need to reshape our figure and can just read them as they are: input is a 2D image with 3 channel, and output is a 2D image with 1 channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWXyJMN48t8P"
   },
   "source": [
    "The dimensions of input tensor x is:\n",
    "\n",
    "> Add blockquote\n",
    "\n",
    "\n",
    "![2018-12-18_09h35_51.png](attachment:2018-12-18_09h35_51.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP-678GX8t8P"
   },
   "source": [
    "The dimensions of our label tensor y_hat is:\n",
    "\n",
    "![2018-12-18_09h36_21.png](attachment:2018-12-18_09h36_21.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gpwXizD8t8P"
   },
   "source": [
    "Since we have our x,y_hat data we can do whatever we want, as long as the input and output dimensions match the datasets dimensions\n",
    "\n",
    "### Layer Definitions:\n",
    "Use the Conv2D layer in tensorflow/keras.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1742459655163,
     "user": {
      "displayName": "Medhansh Agarwal",
      "userId": "03876177846036089524"
     },
     "user_tz": -60
    },
    "id": "NEOLVOf28t8P",
    "outputId": "e7b8a84e-9ab5-4daa-829e-7918f16cee46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\medha\\OneDrive\\Desktop\\AE2224-TAS\\TAS-AE2224-I\\.venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,820</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">181</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │           \u001b[38;5;34m280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m20\u001b[0m)     │         \u001b[38;5;34m1,820\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │           \u001b[38;5;34m181\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,281</span> (8.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,281\u001b[0m (8.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,281</span> (8.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,281\u001b[0m (8.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Define your network here\n",
    "model = tf.keras.Sequential()\n",
    "model.add( tf.keras.layers.Conv2D(10, input_shape=(imwidth,imwidth,3), kernel_size=3,\n",
    "                                  activation='relu', padding='same', strides=1) )\n",
    "model.add( tf.keras.layers.Conv2D(20, kernel_size=3,\n",
    "                                  activation='relu', padding='same', strides=1) )\n",
    "model.add( tf.keras.layers.Conv2D(1, kernel_size=3,\n",
    "                                  activation='sigmoid', padding='same', strides=1) )\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxadEYcz8t8P"
   },
   "source": [
    "## Training the Network\n",
    "We use the difference between the network output and the real image to calculate or cost/loss function. As an optimizier we choose the AdamOptimizer and tell it which function value to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXn8TYPc8t8P",
    "outputId": "37c8e33b-0189-4f84-cf62-5104627e3947"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Need to be filled\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# train your network\u001b[39;00m\n\u001b[0;32m      3\u001b[0m early_stop_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mYY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mep_lim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\medha\\OneDrive\\Desktop\\AE2224-TAS\\TAS-AE2224-I\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\medha\\OneDrive\\Desktop\\AE2224-TAS\\TAS-AE2224-I\\.venv\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py:500\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m    497\u001b[0m split_at \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mfloor(batch_dim \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m validation_split)))\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m batch_dim:\n\u001b[1;32m--> 500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples, which is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msufficient to split it into a validation and training set as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified by `validation_split=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_split\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide more data, or a different value for the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_split` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_split\u001b[39m(t, start, end):\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "## Need to be filled\n",
    "# train your network\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=20)\n",
    "hist = model.fit(XX,YY, batch_size=batchsize, epochs=ep_lim, validation_split=0.2, callbacks=[early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75zJkKmt8t8P"
   },
   "outputs": [],
   "source": [
    "# Monitor the loss function\n",
    "loss_history = hist.history['loss']\n",
    "val_history = hist.history['val_loss']\n",
    "%matplotlib inline\n",
    "plt.plot(loss_history)\n",
    "plt.plot(val_history,'r')\n",
    "plt.title(\"Loss History\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IUxkNh88t8P"
   },
   "outputs": [],
   "source": [
    "# check the prediction of a random sample\n",
    "# use the imf.showImageDataGrey and imf.showImageDataColor to see\n",
    "# the input/output image\n",
    "y_nn = model.predict(XX[0:10,:,:])\n",
    "showImagesDataGrey(y_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ws19Hb_i8t8P"
   },
   "outputs": [],
   "source": [
    "showImagesDataColor(XX[0:10,:,:])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
