{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1633,
     "status": "ok",
     "timestamp": 1742456491471,
     "user": {
      "displayName": "Georgii Miakishev",
      "userId": "00896216475612259069"
     },
     "user_tz": -60
    },
    "id": "NLYDTWbMqPtn",
    "outputId": "3aa91267-148b-4784-8510-c5c030d9661a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# FUNCTIONS\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# FUNCTIONS\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "def saveImages(data, path,epoch = \"\"):\n",
    "    i = 0\n",
    "    add = \"\"\n",
    "\n",
    "    for array in data:\n",
    "        if len(array.shape) == 2:\n",
    "                array2 = np.asarray(array*255 , np.uint8)\n",
    "                array2 = np.reshape(array2,[imwidth,imwidth])\n",
    "                add = \"x\"+str(epoch).zfill(3)\n",
    "                im = Image.fromarray(array2)\n",
    "                im.save(os.path.join(path,add+str(i)+\".png\"))\n",
    "                i = i+1\n",
    "        if len(array.shape)==3:\n",
    "            if array.shape[2] == 1:\n",
    "                array = array[:, :, 0]\n",
    "                add = \"z\"\n",
    "\n",
    "            im = Image.fromarray(array)\n",
    "            im.save(os.path.join(path,add+str(i)+\".png\"))\n",
    "            i = i+1\n",
    "\n",
    "def read_images(input_paths,label_paths):\n",
    "    \"\"\" read filepaths and return decoded images \"\"\"\n",
    "    input_content = tf.io.read_file(input_paths)\n",
    "    label_content = tf.io.read_file(label_paths)\n",
    "    input_images = tf.image.decode_png(input_content, channels=3)\n",
    "    labels = tf.image.decode_png(label_content, channels=1)\n",
    "    return input_images, labels\n",
    "\n",
    "def create_stars_data(input_image, label):\n",
    "    \"\"\" manipulates images as needed for the stars (and similar) dataset \"\"\"\n",
    "    input_image = tf.divide(input_image, np.uint8(255), name=\"normalize_x\")\n",
    "    input_image = tf.image.resize(input_image, [imwidth, imwidth])\n",
    "    #input_image = tf.reshape(input_image,[3,imwidth,imwidth])\n",
    "\n",
    "    label = tf.divide(label, np.uint8(255), name=\"normalize_y\")\n",
    "    label = tf.image.resize(label, [imwidth, imwidth])\n",
    "    label = tf.round(label, name=\"round_y\")\n",
    "    # label = tf.reshape(label,[1,imwidth*imwidth])\n",
    "    return input_image, label\n",
    "\n",
    "def showImagesDataColor(data):\n",
    "            fig=plt2.figure(figsize=(16, 16))\n",
    "            columns = len(data)\n",
    "            rows = 1\n",
    "            i=0\n",
    "            for array in data:\n",
    "                i = i+1\n",
    "                array2 = np.asarray(array*255 , np.uint8)\n",
    "                #array2 = np.reshape(array2,[imwidth,imwidth,3])\n",
    "                array2 = np.reshape(array2,[imwidth,imwidth,3])\n",
    "\n",
    "                fig.add_subplot(rows, columns,i)\n",
    "                plt2.imshow(array2) #,cmap='sRGB1')\n",
    "            plt2.show()\n",
    "\n",
    "def showImagesDataGrey(data):\n",
    "            fig=plt2.figure(figsize=(16, 16))\n",
    "            columns = len(data)\n",
    "            rows = 1\n",
    "            i=0\n",
    "            for array in data:\n",
    "                i = i+1\n",
    "                array2 = np.asarray(array*255 , np.uint8)\n",
    "                array2 = np.reshape(array2,[imwidth,imwidth])\n",
    "\n",
    "                fig.add_subplot(rows, columns,i)\n",
    "                plt2.imshow(array2,cmap='gray')\n",
    "            plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yUEJxtfZ8wc_",
    "outputId": "14f422ef-070c-4f4b-9f34-2648d9bc2320"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!unzip '/content/drive/MyDrive/TAAS24-25/mission3/Stars.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtmWKvbA8t8M"
   },
   "source": [
    ".ZI# ConvStarNet - Visualizing Weights and Activation Maps\n",
    "\n",
    "In this session, we revisit the star identification task seen earlier where we will use CNNs instead of Dense layers. The notebook therefore follows the same structure as the earlier notebook.\n",
    "\n",
    "\n",
    "Input:\n",
    "![2018-11-19_19h52_19.png](attachment:2018-11-19_19h52_19.png)\n",
    "Output:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-4u2zp98t8N"
   },
   "source": [
    "Our neural network should take some image data x and output some image data y with the recognized stars. For training the network we use the star dataset given with this lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mPKHwos8t8N"
   },
   "source": [
    "![2018-12-18_09h33_42.png](attachment:2018-12-18_09h33_42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSDKxguE8t8N"
   },
   "source": [
    "## We have to import some libraries and functions first.\n",
    "Needed libraries are:\n",
    "pillow, tensorflow, numpy, matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wm50PRqv8t8N"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mplatform\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mglob\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os, platform, sys, glob, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDIKcgNB8t8O"
   },
   "source": [
    "We first set up some general hyperparameters for the training and preprocessing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbCmvVmA8t8O"
   },
   "outputs": [],
   "source": [
    "###### Hyperparameter\n",
    "learnrate = 0.0001    # Learning Rate\n",
    "batchsize = 10        # Images per batch\n",
    "ep_lim = 100           # Max number of epochs\n",
    "imwidth = 25          # Image dimension used for training, smaller images computer faster\n",
    "                      # big impact on computation time, original image is 75x75\n",
    "imwidth = imwidth # Set width for imf (image functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5813Kq1I8t8O"
   },
   "source": [
    "## Input data\n",
    "We search the folders containing the data and create two file list, one containing the x and one containing the y file paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1742459647384,
     "user": {
      "displayName": "Medhansh Agarwal",
      "userId": "03876177846036089524"
     },
     "user_tz": -60
    },
    "id": "u9i53WDE8t8O",
    "outputId": "43cedbf0-ce06-445e-b1e5-a72e7342987b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  2350  images\n"
     ]
    }
   ],
   "source": [
    "#### Lisiting all Image files\n",
    "x_path = \"/content/Stars/x\"\n",
    "y_path = \"/content/Stars/y\"\n",
    "file_list_x = []\n",
    "file_list_y = []\n",
    "file_list_x += sorted(glob.glob(x_path + \"/**/*.png\", recursive=True))\n",
    "file_list_y += sorted(glob.glob(y_path + \"/**/*.png\", recursive=True))\n",
    "print(\"Found \",len(file_list_x),\" images\")\n",
    "assert len(file_list_x) == 2350 # If this line failes you did something wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2zkNRQ88t8O"
   },
   "source": [
    "## Tensorflow Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7453,
     "status": "ok",
     "timestamp": 1742459654839,
     "user": {
      "displayName": "Medhansh Agarwal",
      "userId": "03876177846036089524"
     },
     "user_tz": -60
    },
    "id": "DdH1MCEo8t8O",
    "outputId": "70638f29-bde3-48c6-8846-c8392764060f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2350, 25, 25, 3)\n",
      "(2350, 25, 25, 1)\n"
     ]
    }
   ],
   "source": [
    "XX = []\n",
    "YY = []\n",
    "for i in range(len(file_list_x)):\n",
    "    file_x = file_list_x[i]\n",
    "    file_y = file_list_y[i]\n",
    "    X,Y = read_images(file_x,file_y)\n",
    "    X,Y = create_stars_data(X,Y)\n",
    "    XX.append(X.numpy())\n",
    "    YY.append(Y.numpy())\n",
    "XX = np.array(XX)\n",
    "YY = np.array(YY)\n",
    "XX = np.reshape(XX, (-1,25,25,3))\n",
    "YY = np.reshape(YY, (-1,25,25,1))\n",
    "print(XX.shape)\n",
    "print(YY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FFx7A1y8t8P"
   },
   "source": [
    "## Defining our Neural Network and initial values for all Variables\n",
    "Now we can define our neural network. This time, we did not need to reshape our figure and can just read them as they are: input is a 2D image with 3 channel, and output is a 2D image with 1 channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWXyJMN48t8P"
   },
   "source": [
    "The dimensions of input tensor x is:\n",
    "\n",
    "> Add blockquote\n",
    "\n",
    "\n",
    "![2018-12-18_09h35_51.png](attachment:2018-12-18_09h35_51.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP-678GX8t8P"
   },
   "source": [
    "The dimensions of our label tensor y_hat is:\n",
    "\n",
    "![2018-12-18_09h36_21.png](attachment:2018-12-18_09h36_21.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gpwXizD8t8P"
   },
   "source": [
    "Since we have our x,y_hat data we can do whatever we want, as long as the input and output dimensions match the datasets dimensions\n",
    "\n",
    "### Layer Definitions:\n",
    "Use the Conv2D layer in tensorflow/keras.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1742459655163,
     "user": {
      "displayName": "Medhansh Agarwal",
      "userId": "03876177846036089524"
     },
     "user_tz": -60
    },
    "id": "NEOLVOf28t8P",
    "outputId": "e7b8a84e-9ab5-4daa-829e-7918f16cee46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,820</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">181</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m10\u001b[0m)          │             \u001b[38;5;34m280\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │           \u001b[38;5;34m1,820\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │             \u001b[38;5;34m181\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,281</span> (8.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,281\u001b[0m (8.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,281</span> (8.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,281\u001b[0m (8.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Define your network here\n",
    "model = tf.keras.Sequential()\n",
    "model.add( tf.keras.layers.Conv2D(10, input_shape=(imwidth,imwidth,3), kernel_size=3,\n",
    "                                  activation='relu', padding='same', strides=1) )\n",
    "model.add( tf.keras.layers.Conv2D(20, kernel_size=3,\n",
    "                                  activation='relu', padding='same', strides=1) )\n",
    "model.add( tf.keras.layers.Conv2D(1, kernel_size=3,\n",
    "                                  activation='sigmoid', padding='same', strides=1) )\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxadEYcz8t8P"
   },
   "source": [
    "## Training the Network\n",
    "We use the difference between the network output and the real image to calculate or cost/loss function. As an optimizier we choose the AdamOptimizer and tell it which function value to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXn8TYPc8t8P",
    "outputId": "37c8e33b-0189-4f84-cf62-5104627e3947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.1324 - val_loss: 0.0346\n",
      "Epoch 2/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0325 - val_loss: 0.0280\n",
      "Epoch 3/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0267 - val_loss: 0.0231\n",
      "Epoch 4/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0221 - val_loss: 0.0196\n",
      "Epoch 5/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0192 - val_loss: 0.0174\n",
      "Epoch 6/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0171 - val_loss: 0.0160\n",
      "Epoch 7/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0158 - val_loss: 0.0152\n",
      "Epoch 8/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0149 - val_loss: 0.0143\n",
      "Epoch 9/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0143 - val_loss: 0.0136\n",
      "Epoch 10/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0137 - val_loss: 0.0133\n",
      "Epoch 11/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0133 - val_loss: 0.0128\n",
      "Epoch 12/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 0.0128 - val_loss: 0.0124\n",
      "Epoch 13/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0124 - val_loss: 0.0122\n",
      "Epoch 14/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0123 - val_loss: 0.0119\n",
      "Epoch 15/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0119 - val_loss: 0.0117\n",
      "Epoch 16/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0118 - val_loss: 0.0114\n",
      "Epoch 17/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0115 - val_loss: 0.0111\n",
      "Epoch 18/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 19/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 20/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 21/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 22/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 23/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 24/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 25/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0098 - val_loss: 0.0097\n",
      "Epoch 26/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0097 - val_loss: 0.0100\n",
      "Epoch 27/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0093 - val_loss: 0.0095\n",
      "Epoch 28/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 29/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0094 - val_loss: 0.0091\n",
      "Epoch 30/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0094 - val_loss: 0.0091\n",
      "Epoch 31/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0094 - val_loss: 0.0089\n",
      "Epoch 32/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 33/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0090 - val_loss: 0.0087\n",
      "Epoch 34/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 35/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 36/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 37/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 38/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0086 - val_loss: 0.0087\n",
      "Epoch 39/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0085 - val_loss: 0.0088\n",
      "Epoch 40/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 41/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 42/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 43/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0087 - val_loss: 0.0083\n",
      "Epoch 44/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 45/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0083 - val_loss: 0.0086\n",
      "Epoch 46/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0082 - val_loss: 0.0083\n",
      "Epoch 47/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 48/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 49/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 50/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 51/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 52/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 53/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 54/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0080 - val_loss: 0.0081\n",
      "Epoch 55/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 56/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 57/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 58/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - loss: 0.0078 - val_loss: 0.0080\n",
      "Epoch 59/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 60/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0081 - val_loss: 0.0078\n",
      "Epoch 61/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 62/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 63/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 64/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 65/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - loss: 0.0078 - val_loss: 0.0081\n",
      "Epoch 66/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0078 - val_loss: 0.0081\n",
      "Epoch 67/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 68/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 69/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0077 - val_loss: 0.0078\n",
      "Epoch 70/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 71/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0076 - val_loss: 0.0083\n",
      "Epoch 72/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 73/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0075 - val_loss: 0.0077\n",
      "Epoch 74/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 75/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 76/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 77/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0075 - val_loss: 0.0077\n",
      "Epoch 78/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0077 - val_loss: 0.0078\n",
      "Epoch 79/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 80/100\n",
      "\u001b[1m176/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0076"
     ]
    }
   ],
   "source": [
    "## Need to be filled\n",
    "# train your network\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=20)\n",
    "hist = model.fit(XX,YY, batch_size=batchsize, epochs=ep_lim, validation_split=0.2, callbacks=[early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75zJkKmt8t8P"
   },
   "outputs": [],
   "source": [
    "# Monitor the loss function\n",
    "loss_history = hist.history['loss']\n",
    "val_history = hist.history['val_loss']\n",
    "%matplotlib inline\n",
    "plt.plot(loss_history)\n",
    "plt.plot(val_history,'r')\n",
    "plt.title(\"Loss History\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IUxkNh88t8P"
   },
   "outputs": [],
   "source": [
    "# check the prediction of a random sample\n",
    "# use the imf.showImageDataGrey and imf.showImageDataColor to see\n",
    "# the input/output image\n",
    "y_nn = model.predict(XX[0:10,:,:])\n",
    "showImagesDataGrey(y_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ws19Hb_i8t8P"
   },
   "outputs": [],
   "source": [
    "showImagesDataColor(XX[0:10,:,:])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
